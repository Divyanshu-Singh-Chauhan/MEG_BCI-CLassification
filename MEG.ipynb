{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "MEG ",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WNZXWfyU0CDl-YYacjTr30LJyDB5IR-Q",
      "authorship_tag": "ABX9TyN2Ox3NeK6mKf18qnJtn4J6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyanshu-Singh-Chauhan/MEG_BCI-CLassification/blob/main/MEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4qMKrrshV5"
      },
      "source": [
        "import time \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, normalize\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import Input, Flatten\n",
        "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, SpatialDropout2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A74YIJ_HcYHC"
      },
      "source": [
        "### LOAD THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqW6Fw6GsoJl"
      },
      "source": [
        "!wget -O train.mat https://www.dropbox.com/s/rki1puhd2lwn2md/ParsedMEGData_P13_S1.mat?dl=0\n",
        "train_annots = loadmat('train.mat')\n",
        "\n",
        "!wget -O test.mat https://www.dropbox.com/s/swcs09t7yb5dbbh/ParsedMEGData_P13_S2.mat?dl=0\n",
        "test_annots = loadmat('test.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBWxo4MOVsiN"
      },
      "source": [
        "### SOME CONTANTS RELATED TO THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxnnI9kTV0s_"
      },
      "source": [
        "new = True\n",
        "subject = '13'\n",
        "img_path = '/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/Curves/P'+ subject\n",
        "path = '/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/P'+subject+'_M'\n",
        "\n",
        "\n",
        "fs = 500                  # sampling frequency = 500 Hz\n",
        "t0 = 2.0                  # Starting Seconds to ignore in the data\n",
        "t_stamp = int(fs*t0)      # 1000 = fs*t0\n",
        "label_dict = {1:'Hand',\n",
        "              2:'Feet',\n",
        "              3:'Word',\n",
        "              4:'Math'}\n",
        "\n",
        "### List of all the cortexes by their serial number\n",
        "MCids= ['064','062','103','041','042','063','104','111','112','044','043','071','072','114','113','181','182','074','073','221','222','183','224']\n",
        "LTids=['031','011','012','034','032','033','013','021','022','014','151','024','023','154','152','161','162','153','172','164','163']\n",
        "RTids=['121','124','123','122','141','131','132','144','142','134','133','261','143','262','241','242','264','263','244','243','252']\n",
        "Fids=['052','081','091','051','053','082','094','092','054','061','101','102','093']\n",
        "Occids=['184','201','202','223','191','204','203','231','194','192','211','234','232','173','193','212','233','251','174','214','213','254']      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9LNfY2yH8N"
      },
      "source": [
        "def data_understand(annots: dict) -> list:\n",
        "\n",
        "    \"\"\"Function for understanding the dataset-->.mat file\n",
        "    \n",
        "    :*param* dicts: The dictionary conatining the dataset\n",
        "    :*returns* processed data and target arrays\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"1. The keys of the dataset are: \", annots.keys())\n",
        "\n",
        "    data = np.asarray(annots['MEGdata'][0][0][1])             ### annots['MEGdata'][0][0][1] =  samples per trial * channels * trials\n",
        "    (s_t, ch, trials) = data.shape\n",
        "    data = data.reshape(trials, ch, s_t)\n",
        "    print(\"2. trials, channels, samples per trial a.k.a data shape is: \", trials, ch, s_t)\n",
        "\n",
        "    labels = np.asarray(annots['MEGdata'][0][0][2])            ### labels per trial\n",
        "    print(\"3. labels shape is: \", labels[0].shape)\n",
        "        \n",
        "    cds = annots['MEGdata'][0][0][3]               ### Channel Codes\n",
        "    codes = []\n",
        "    for i in range(cds.shape[0]):\n",
        "      codes.append(cds[i][0][0])\n",
        "    codes = np.asarray(codes)                      ### array of strings of the channel codes .. eg --> 'MEG0123'\n",
        "    print(\"4. Codes shape is: \", codes.shape)\n",
        "\n",
        "    serial_n = np.asarray(annots['MEGdata'][0][0][4])          ### Channels Serial Number\n",
        "    print(\"5. Serial_n shape is: \", serial_n.shape)\n",
        "\n",
        "    classes_names = np.asarray(annots['MEGdata'][0][0][5])     ### Annotations of classes\n",
        "    print(\"6. names of the classes are:\", classes_names)  \n",
        "\n",
        "    return data, labels, codes, serial_n, classes_names\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "# # Script for considering experiment from 2.5 seconds to 5.5 seconds of data \n",
        "def remove_time(X: list)->list:\n",
        "  \"\"\"Function for removing the first t0 seconds from the data\n",
        "\n",
        "  :*param* X: input data array\n",
        "  :*returns* data with removed elements \n",
        "  \"\"\"\n",
        "  return X[:,:,int(fs*2.5):int(fs*5.5)]\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "### CHANGE THE LABELS ONLY TO TRAIN A DIFFERENT PAIR\n",
        "def get_data(labels: list, X: list, class_list: list):\n",
        "\n",
        "    \"\"\"Function to get the data for the given two classes\n",
        "\n",
        "    :label 1 -> Hand\n",
        "    :Label 2 -> Feet\n",
        "    :Label 3 -> Word\n",
        "    :Label 4 -> Math\n",
        "\n",
        "    :*param* labels: Target labels \n",
        "    :*param* X: Contains the input data\n",
        "    :*param* class_list: contains 2 elements for each class required\n",
        "    :*returns* binary class data from the dataset\n",
        "    \"\"\"\n",
        "    l1 = class_list[0]\n",
        "    l2 = class_list[1]\n",
        "\n",
        "    print(\"The data being considered is for\",label_dict[l1],\"vs\",label_dict[l2])\n",
        "    ind1_arr = np.where(labels[0]==l1)\n",
        "    X1 = []\n",
        "    for i in ind1_arr[0]:\n",
        "      X1.append(X[i])\n",
        "    X1 = np.asarray(X1)\n",
        "    Y1 = np.zeros(50)\n",
        "\n",
        "    ind2_arr = np.where(labels[0]==l2)\n",
        "    X2 = []\n",
        "    for i in ind2_arr[0]:\n",
        "      X2.append(X[i])\n",
        "    X2 = np.asarray(X2)\n",
        "    Y2 = np.ones(50)\n",
        "\n",
        "    X = np.concatenate((X1,X2), axis = 0)\n",
        "    Y = np.concatenate((Y1,Y2), axis = 0)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "\"------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "def get_encodings(Y: list)->list:\n",
        "\n",
        "    \"\"\"Function for transforming target data to one hot encodings\n",
        "\n",
        "    :*param* Y: Arrays containing target binary labels\n",
        "    :*returns* transformed one hot encoded array \n",
        "    \"\"\"\n",
        "\n",
        "    ### CONVERT LABEL OUTPUT TO A ONE HOT ENCODING i.e the shape would be (200,2)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(Y)\n",
        "    Y = encoder.transform(Y)\n",
        "    # convert integers to dummy variables (i.e. one hot encoded)\n",
        "    Y = np_utils.to_categorical(Y)\n",
        "    return Y\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"    \n",
        "\n",
        "def get_band(band: str)->float:\n",
        "\n",
        "    \"\"\"Function to get the frequency band\n",
        "\n",
        "    :*param* band: string denoting the band\n",
        "    :*returns* the lower and upper bound of frequency\n",
        "    \"\"\"\n",
        "    if band=='alpha':\n",
        "      lowcut = 8\n",
        "      highcut = 12\n",
        "    elif band == 'beta':\n",
        "      lowcut = 14\n",
        "      highcut = 30\n",
        "    elif band == 'mu':\n",
        "      lowcut = 12\n",
        "      highcut = 16\n",
        "    elif band == 'ab':\n",
        "      lowcut = 8\n",
        "      highcut = 30\n",
        "    else:\n",
        "      lowcut = 0.5\n",
        "      highcut = 4\n",
        "    \n",
        "    return lowcut,highcut\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "\n",
        "def temporal_filtering(data: list, order: int, band = 'ab')->list:\n",
        "\n",
        "    \"\"\"Function to perform temporal filtering\n",
        "\n",
        "    :*param* data: input array to be filtered\n",
        "    :*param* order: generally kept as 4 for current task\n",
        "    :*param* band: denoting the band of frequency required\n",
        "    :*returns* the temporally filtered signal\n",
        "    \"\"\"\n",
        "    lowcut, highcut = get_band(band)\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    filtered = lfilter(b, a, data)\n",
        "    return filtered\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "# Script for sorting channels into their cortexes and taking only gradiometers\n",
        "def get_channels(names: list, grad = True)->dict:\n",
        "\n",
        "    \"\"\"Function to get the lists of indexes for channels in each cortex\n",
        "\n",
        "\n",
        "    :*param* name: array of strings containing codes \n",
        "    :*param* grad: bool True signifies that we are taking only gradiometers \n",
        "    :*returns* dictionary containing the lists for each cortex\n",
        "    \"\"\"\n",
        "    dicts = {}\n",
        "    dicts['Motor'] = []\n",
        "    dicts['Left'] = []\n",
        "    dicts['Right'] = []\n",
        "    dicts['Frontal'] = []\n",
        "    dicts['Occids'] = []\n",
        "    ls = []\n",
        "    for i in range(len(names)):\n",
        "      name = names[i]\n",
        "      n_code = name[3:7]      # numeric code of the channel\n",
        "      ids = name[3:6]\n",
        "      if grad == True:\n",
        "        if n_code[-1] != '1':\n",
        "          if ids in MCids:\n",
        "            dicts['Motor'].append(i)\n",
        "          elif ids in LTids:\n",
        "            dicts['Left'].append(i)\n",
        "          elif ids in RTids:\n",
        "            dicts['Right'].append(i)\n",
        "          elif ids in Fids:\n",
        "            dicts['Frontal'].append(i)\n",
        "          else:\n",
        "            dicts['Occids'].append(i)\n",
        "      \n",
        "      else:\n",
        "        if ids in MCids:\n",
        "          dicts['Motor'].append(i)\n",
        "        elif ids in LTids:\n",
        "          dicts['Left'].append(i)\n",
        "        elif ids in RTids:\n",
        "          dicts['Right'].append(i)\n",
        "        elif ids in Fids:\n",
        "          dicts['Frontal'].append(i)\n",
        "        else:\n",
        "          dicts['Occids'].append(i)\n",
        "        \n",
        "    return dicts\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "\n",
        "def remove_channel(X: list, dicts: dict)->list:\n",
        "\n",
        "  \"\"\"Function to get the remove the unnecessary channels from the data\n",
        "\n",
        "  :*param* X: input filtered array\n",
        "  :*param* dicts: dictionary containing the channel indexes\n",
        "  :*returns* input array with removed channels\n",
        "  \"\"\"\n",
        "\n",
        "  M_ids = dicts['Motor']\n",
        "  L_ids = dicts['Left']\n",
        "  R_ids = dicts['Right']\n",
        "  F_ids = dicts['Frontal']\n",
        "  O_ids = dicts['Occids']\n",
        "\n",
        "  l1 = []\n",
        "  for i in range(X.shape[0]):\n",
        "    l2 = []\n",
        "    for j in (M_ids+L_ids+R_ids+F_ids+O_ids):                               ### Currently taking Mortex cortex channels\n",
        "      l3 = []\n",
        "      for k in range(X.shape[2]):\n",
        "        val = X[i][j][k]\n",
        "        l3.append(val)\n",
        "      l2.append(l3)\n",
        "    l1.append(l2)\n",
        "  return np.asarray(l1)\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "def min_max_normalized(X: list)->list:\n",
        "\n",
        "  \"\"\"Function to get normalized data and an increased dimension at the last axis\n",
        "\n",
        "  :*param* X: input data array to be normalized\n",
        "  :*returns normalized input array\n",
        "  \"\"\"\n",
        "  for i in range(X.shape[0]):\n",
        "    X[i] = normalize(X[i])\n",
        "  X = np.expand_dims(X, axis=-1)\n",
        "  return X\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "def z_score_normalized(X:list)->list:\n",
        "  \"\"\"Function to get z-score normalized data at the last axis\n",
        "\n",
        "  :*param* X: input data array to be z-score normalized\n",
        "  :*returns* z-score normalized array \n",
        "  \"\"\"\n",
        "  for i in range(X.shape[0]):\n",
        "    X[i] = stats.zscore(X[i], axis=1)\n",
        "  X = np.expand_dims(X,axis=-1)\n",
        "  return X\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "def visualize(X_hand:list, X_feet:list):\n",
        "  x = np.linspace(0,X_hand.shape[0],X_hand.shape[0])\n",
        "  print(x.shape)\n",
        "  fig, (ax1, ax2) = plt.subplots(2)\n",
        "  fig.suptitle('Horizontally stacked subplots')\n",
        "  ax1.plot(x, X_hand)\n",
        "  ax1.set_title(\"Hand\")\n",
        "  ax2.plot(x, X_feet)\n",
        "  ax2.set_title(\"Feet\")\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "\n",
        "def prep_data(annots, train = True):\n",
        "\n",
        "  data, labels, codes, serial_n, classes_names = data_understand(annots)\n",
        "  X_reduced = remove_time(data)\n",
        "\n",
        "  X_binary,Y_binary = get_data(labels, X_reduced, [1,2])\n",
        "\n",
        "  Y = get_encodings(Y_binary)\n",
        "\n",
        "  X_ab = temporal_filtering(X_binary, 4, band = 'ab')\n",
        "  X_filtered = X_ab\n",
        "\n",
        "  this_dict = get_channels(codes, grad = True)\n",
        "  X_removed = remove_channel(X_filtered, this_dict)\n",
        "\n",
        "  X = z_score_normalized(X_removed)\n",
        "\n",
        "  if train == True:\n",
        "    mode = 'Training'\n",
        "\n",
        "  else:\n",
        "    mode = 'Testing'\n",
        "\n",
        "  np.save('/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/Data Arrays/'+mode+'/X_P'+subject, X)\n",
        "  np.save('/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/Data Arrays/'+mode+'/Y_P'+subject, Y)\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"  \n",
        "\n",
        "\n",
        "def eval(X: list, Y: list):\n",
        "    \"\"\"Function to evaluate on test data\n",
        "\n",
        "    *param* X: Input data list \n",
        "    *param* Y: Target labels \n",
        "\n",
        "    *prints* the test accuracy and test loss scores\n",
        "    \"\"\"\n",
        "\n",
        "    score = model.evaluate(X, Y, verbose = 1) \n",
        "\n",
        "    print('loss:', score[0]) \n",
        "    print('Accuracy:', score[1])\n",
        "\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    probs = model.predict(X)\n",
        "    print(accuracy_score(Y.argmax(axis=1), probs.argmax(axis=1)))\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "def plot_learning(results):\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.title(\"Learning curve\")\n",
        "    plt.plot(results.history[\"loss\"], label=\"loss\")\n",
        "    plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"log_loss\")\n",
        "    plt.savefig(img_path+'_LR')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "def plot_roc_curve(Y_predicted: list, Y_actual: list):\n",
        "    \"\"\"Function to plot the ROC curve and give AUC value\n",
        "\n",
        "    :*param* Y_predict: predicted model output in float values\n",
        "    :*param* Y_actual: actual labels\n",
        "    :*returns* the ROC plot and AUC values\n",
        "    \"\"\"\n",
        "    # calculate roc curve\n",
        "    fpr, tpr, thresholds = roc_curve(Y_train.argmax(axis=1), probs.argmax(axis=1))\n",
        "\n",
        "    # calculate AUC\n",
        "    auc = roc_auc_score(Y_train.argmax(axis=1), probs.argmax(axis=1))\n",
        "    print('AUC: %.3f' % auc)\n",
        "\n",
        "    plt.plot(fpr, tpr, marker='.', label='EEGNet')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend()\n",
        "    plt.savefig(img_path+'_ROC')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6KUtywa6ujc"
      },
      "source": [
        "if new == True:\n",
        "    X_train, Y_train = prep_data(train_annots, train = True)\n",
        "    X_test, Y_test = prep_data(test_annots, train = False)\n",
        "\n",
        "    print(\"Train shape is:\", X_train.shape)\n",
        "    print(\"Test shape is:\", X_test.shape)\n",
        "\n",
        "else:\n",
        "    X_train =  np.load('/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/Data Arrays/Training/X_P'+subject+'.npy')\n",
        "    Y_train =  np.load('/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/Data Arrays/Training/Y_P'+subject+'.npy')\n",
        "\n",
        "    X_test =  np.load('/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/Data Arrays/Testing/X_P'+subject+'.npy')\n",
        "    Y_test =  np.load('/content/drive/MyDrive/U_Essex/Final_Model_Weights/MEG/Data Arrays/Testing/Y_P'+subject+'.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss1QKI6N_fru"
      },
      "source": [
        "#### CALLBACKS ####\n",
        "\n",
        "\n",
        "### MODELCHECKPOINT CALLBACK\n",
        "save = ModelCheckpoint(path+ '.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "### REDUCES LR WHEN METRTIC IS NOT IMPROVING\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_delta =1e-4, min_lr=0.00001, mode='min')\n",
        "\n",
        "### LEARNING RATE SCHEDULER\n",
        "def scheduler(epoch):\n",
        "\n",
        "  if epoch < 5:\n",
        "     return 0.001\n",
        "  else:\n",
        "     return float(0.001 * tf.math.exp(0.1 * (5 - epoch)))\n",
        "lr_schedule = LearningRateScheduler(scheduler) \n",
        "\n",
        "### EARLY STOPPING\n",
        "early_stopping = EarlyStopping(patience=10, verbose=1)\n",
        "\n",
        "\n",
        "\"---------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "def EEGNet(nb_classes, chans=64, samples = 128, dropoutRate = 0.5,\n",
        "           kernLength = 32, F1 = 8, D = 2, F2 = 16, norm_rate = 0.25,\n",
        "           dropoutType = 'Dropout'):\n",
        "  \"\"\"\n",
        "  Keras implementation of EEGNet \n",
        "  Parameters:\n",
        "\n",
        "  nb_classes     : int, number of classes to classify\n",
        "  chans, samples : number of channels and time stamps in the EEG signal\n",
        "  dropoutRate    : dropout fraction\n",
        "  kernLength     : length of temporal convolutions in 1st layer.\n",
        "  F1, F2         : # of temporal and pointwise filters to learn. Default: F1 = 8, F2 = F1*D\n",
        "  D              : # of spatial filters to learn withing each temporal convolution. Default: D = 2\n",
        "  droputType     : Either SpatialDropout2D or Dropout passed as string\n",
        "  \"\"\"\n",
        "\n",
        "  if dropoutType == 'SpatialDropout2D':\n",
        "    dropoutType = SpatialDropout2D\n",
        "  elif dropoutType == 'Dropout':\n",
        "    dropoutType = Dropout\n",
        "  else:\n",
        "    raise ValueError('dropout can either be SpatialDropout2D or Dropout, passed as a string.')\n",
        "\n",
        "  input1 = Input(shape=(chans, samples, 1))\n",
        "\n",
        "  # Model Architecture\n",
        "  block1 = Conv2D(F1, (1, kernLength), padding = 'same',\n",
        "                  input_shape = (chans, samples, 1), \n",
        "                  use_bias = False)(input1)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = DepthwiseConv2D((chans, 1), use_bias = False, \n",
        "                           depth_multiplier = D,\n",
        "                           depthwise_constraint = max_norm(1.))(block1)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = Activation('elu')(block1)\n",
        "  block1 = AveragePooling2D((1,4))(block1)\n",
        "  block1 = dropoutType(dropoutRate)(block1)\n",
        "\n",
        "\n",
        "  block2 = SeparableConv2D(F2, (1,16), \n",
        "                           use_bias = False,\n",
        "                           padding = 'same')(block1)\n",
        "  block2 = BatchNormalization()(block2)\n",
        "  block2 = Activation('elu')(block2)\n",
        "  block2 = AveragePooling2D((1,8))(block2)\n",
        "  block2 = dropoutType(dropoutRate)(block2)\n",
        "\n",
        "  flatten = Flatten(name = 'flatten')(block2)\n",
        "  dense = Dense(nb_classes, name = 'Dense', \n",
        "                kernel_constraint = max_norm(norm_rate))(flatten)\n",
        "  softmax = Activation('softmax', name = 'softmax')(dense)\n",
        "\n",
        "  model = Model(inputs=input1, outputs = softmax)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\"------------------------------------------------------------------------------\"\n",
        "\n",
        "\n",
        "\n",
        "model = EEGNet(nb_classes = 2, chans = X_train.shape[1], samples = X_train.shape[2],\n",
        "                             dropoutRate = 0.5, kernLength = 32, F1 = 8,D = 2, F2 = 16,\n",
        "                             norm_rate = 0.25, dropoutType = 'Dropout')\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = [\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0thOscxbFM6"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "X_train1, Y_train1 = shuffle(X_train, Y_train)\n",
        "print(X_train1.shape)\n",
        "print(Y_train1.shape)\n",
        "\n",
        "\n",
        "trainX = X_train1[0:75,:,:]\n",
        "trainY = Y_train1[0:75]\n",
        "testX = X_train1[75:,:,:]\n",
        "testY = Y_train1[75:]\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)\n",
        "print(testX.shape)\n",
        "print(testY.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxAnze7I89H6"
      },
      "source": [
        "# model.load_weights(path+ '.h5')\n",
        "class_weights = {0:1, 1:1}\n",
        "results = model.fit(trainX, trainY,batch_size=16, epochs=200, validation_data = (testX, testY), callbacks=[save], class_weight = class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAZPul8lCxTB"
      },
      "source": [
        "plot_learning(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjpDeLiFVNZ4"
      },
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUAeKrtKfwTu"
      },
      "source": [
        "print(\"TEST METRICS\")\n",
        "eval(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjrklKJf5MwK"
      },
      "source": [
        "print(\"TRAIN METRICS\")\n",
        "eval(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}